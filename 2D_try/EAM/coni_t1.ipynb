{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matscipy\n",
    "from matscipy import calculators\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from copy import copy\n",
    "from itertools import combinations\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from scipy import constants\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def supercell_gen(cell_num1, cell_num2):\n",
    "    atom_pos1 = cell_num1*2 + 1\n",
    "    atom_pos2 = cell_num2*2 + 1\n",
    "    zero_cell = np.zeros(((atom_pos1, atom_pos2)))\n",
    "    for i in range(atom_pos1):\n",
    "        for j in range(atom_pos2):\n",
    "            if i%2 == 1:\n",
    "                zero_cell[i][j] = abs(j%2)\n",
    "            elif i%2 == 0:\n",
    "                zero_cell[i][j] = 1 - abs(j%2)\n",
    "    return zero_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Read NC's potential for CrCoNi\n",
    "eam_pth = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/pyMD/2D_try/EAM/NiCoCr.lammps_t1.eam'\n",
    "_, param, f_s, rhof_s, phi_r = calculators.eam.io.read_eam(eam_pth, kind='eam/alloy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters based on Zhou et al's work\n",
    "\n",
    "https://journals.aps.org/prb/abstract/10.1103/PhysRevB.69.144113\n",
    "\n",
    "### Main:\n",
    "$$E = 1/2\\sum_{i \\neq j}\\phi_{ij}(r_{ij}) + \\sum_{i}F_i(\\rho_i) \\tag{1-1}$$\n",
    "\n",
    "$$f(r) = \\frac{f_e e^{-\\beta(r/r_e-1)}}{1+(r/r_e-\\lambda)^{20}} \\tag{1-2}$$\n",
    "\n",
    "*variable: f_e, beta, lamda*\n",
    "\n",
    "#### Potential\n",
    "\n",
    "$$\\phi(r) = \\frac{Ae^{-\\alpha(r/r_e-1)}}{1+(r/r_e-\\kappa)^{20}}-\\frac{Be^{-\\beta(r/r_e-1)}}{1+(r/r_e-\\lambda)^{20}} \\tag{2-1}$$\n",
    "\n",
    "*variable: A, B, alpha, beta, kappa, lamda, r_e*\n",
    "\n",
    "$$\\phi^{ab}(r) = 1/2[\\frac{f^b(r)}{f^a(r)}\\phi^{aa}(r)+\\frac{f^a(r)}{f^b(r)}\\phi^{bb}(r)] \\tag{2-2}$$\n",
    "\n",
    "#### Density of electron\n",
    "\n",
    "$$\\rho_i = \\sum_{j \\neq i}f_j(r_{ij}) \\tag{3-1}$$\n",
    "\n",
    "$$F(\\rho) = \n",
    "\\begin{cases}\n",
    "\\sum_{i=0}^{3}F_{ni}(\\rho/\\rho_n-1)^i,\\ \\rho < \\rho_n,\\ \\rho_n = 0.85\\rho_e \\\\\n",
    "\\sum_{i=0}^{3}F_{i}(\\rho/\\rho_e-1)^i,\\ \\rho_n \\leq \\rho < \\rho_0,\\ \\rho_0 = 1.15\\rho_e\\\\\n",
    "F_e[1-\\mathrm{ln}(\\rho/\\rho_s)^{\\eta}](\\rho/\\rho_s)^{\\eta},\\ \\rho_0 \\leq \\rho\n",
    "\\end{cases} \\tag{3-2}$$\n",
    "\n",
    "*variable: rho_e, rho_s, eta, f_n0~f_n3, f_0~f_3*\n",
    "\n",
    "| | r_e | f_e | rho_e | rho_s | alpha | beta | A | B | kappa | lamda |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| INDEX | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| | f_no | f_n1 | f_n2 | f_n3 | f_0 | f_1 | f_2 | f_3 | eta | f_e | rho_n | rho_0 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| INDEX | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, atom_list, param_list):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        atom_list: N*dimension\n",
    "        param_list: N*number of params per each atom\n",
    "        per each row:\n",
    "        r_e, f_e, rho_e, rho_s, alpha, beta, A, B, kappa, lamda,\n",
    "        f_n0, f_n1, f_n2, f_n3, f_0, f_1, f_2, f_3, eta, f_e\n",
    "        '''\n",
    "        #* row1: x-coord, row2: y-coord\n",
    "        self.weights = nn.Parameter(atom_list)\n",
    "        self.weights_ = self.weights.clone()\n",
    "        atom_num = len(atom_list)\n",
    "\n",
    "        self.ind_inter = torch.combinations(torch.arange(self.weights.size(0)), r=2).to(device)\n",
    "        ind_inter_f = torch.fliplr(self.ind_inter)\n",
    "        ind_all = torch.cat((self.ind_inter, ind_inter_f), dim=0)\n",
    "        ind_all = ind_all[ind_all[:, 0].sort()[1]]\n",
    "        self.ind_block = torch.split(ind_all, atom_num-1)\n",
    "\n",
    "        self.rho_list = torch.zeros(atom_num)\n",
    "        self.range = torch.arange(atom_num)\n",
    "        self.params = param_list\n",
    "\n",
    "    def f_r(self, r, param):\n",
    "        '''\n",
    "        Basis function for f(r), tag 1-2\n",
    "        Param is nD\n",
    "        '''\n",
    "        f_e, beta, r_e, lamda = torch.tensor_split(param, param.size(1), dim=1)\n",
    "        f_e, beta, r_e, lamda = torch.flatten(f_e), torch.flatten(beta), torch.flatten(r_e), torch.flatten(lamda)\n",
    "\n",
    "        nume = f_e*torch.exp(-beta*(r/r_e-1))\n",
    "        deno = 1 + (r/r_e-lamda)**20\n",
    "        return nume/deno \n",
    "\n",
    "    def phi_r(self, r, param):\n",
    "        ''' \n",
    "        Potential for tag 2-1, atomic pair for same species.\n",
    "        Param is nD.\n",
    "        '''\n",
    "        a, b, r_e, alpha, beta, kappa, lamda = torch.tensor_split(param, param.size(1), dim=1)\n",
    "        a, b = torch.flatten(a), torch.flatten(b)\n",
    "        r_e, alpha, beta = torch.flatten(r_e), torch.flatten(alpha), torch.flatten(beta)\n",
    "        kappa, lamda = torch.flatten(kappa), torch.flatten(lamda)\n",
    "\n",
    "        l_nume = a*torch.exp(-alpha*(r/r_e-1))\n",
    "        l_deno = 1 + (r/r_e-kappa)**20\n",
    "        r_nume = b*torch.exp(-beta*(r/r_e-1))\n",
    "        r_deno = 1 + (r/r_e-lamda)**20\n",
    "\n",
    "        return l_nume/l_deno - r_nume/r_deno\n",
    "\n",
    "    def f_rho(self, rho, param):\n",
    "        ''' \n",
    "        Density function for tag 3-2, param is 1D for the centre atom\n",
    "        '''\n",
    "        f_n0, f_n1, f_n2, f_n3, f_0, f_1, f_2, f_3, f_e, rho_n, rho_e, rho_0, rho_s, eta = param\n",
    "        if rho < rho_n:\n",
    "            return (f_n0 + f_n1*(rho/rho_n-1)\n",
    "                + f_n2*(rho/rho_n-1)**2 + f_n3*(rho/rho_n-1)**3)\n",
    "        elif rho_n <= rho < rho_0:\n",
    "            return (f_0 + f_1*(rho/rho_e-1)\n",
    "                + f_2*(rho/rho_e-1)**2 + f_3*(rho/rho_e-1)**3)\n",
    "        else:\n",
    "            return f_e*(1-torch.log((rho/rho_s)**eta))*(rho/rho_s)**eta\n",
    "\n",
    "    def forward(self,):\n",
    "        ''' \n",
    "        Calculate the whole energy with keeping gradient\n",
    "        '''\n",
    "        coord_inter = self.weights[self.ind_inter]\n",
    "        param_inter = self.params[self.ind_inter]\n",
    "        r_res = torch.norm(coord_inter[:,1]-coord_inter[:,0], dim=1)\n",
    "        #* So this will be the cutoff ?\n",
    "        effe_r_ind = torch.nonzero(r_res <= 16)\n",
    "        r_res = r_res[torch.flatten(effe_r_ind)]\n",
    "        param_inter = param_inter[torch.flatten(effe_r_ind)]\n",
    "\n",
    "        fr_0 = self.f_r(r_res, param_inter[:, 0][:, [1, 5, 0, 9]]) #* For f^0(r)\n",
    "        fr_1 = self.f_r(r_res, param_inter[:, 1][:, [1, 5, 0, 9]]) #* For f^1(r)\n",
    "        phir_0 = self.phi_r(r_res, param_inter[:, 0][:, [6,7,0,4,5,8,9]]) #* For phi^0(r)\n",
    "        phir_1 = self.phi_r(r_res, param_inter[:, 1][:, [6,7,0,4,5,8,9]]) #* For phi^1(r)\n",
    "        phi_01 = fr_1/fr_0*phir_0 + fr_0/fr_1*phir_1 #* For phi^01\n",
    "        p_e = 1/2*torch.sum(phi_01) #* Potential energy term\n",
    "\n",
    "        for r_ind in self.range:\n",
    "            '''\n",
    "            Extract each atom block containing the centre i and surronding j\n",
    "            '''\n",
    "            block = self.ind_block[r_ind]\n",
    "            coord_block = self.weights[block]\n",
    "            param_block = self.params[block]\n",
    "            r_blo = torch.norm(coord_block[:,1]-coord_block[:,0], dim=1)\n",
    "            #* So this will be the cutoff ~5 NN?\n",
    "            effe_r_ind = torch.nonzero(r_blo <= 16)\n",
    "            r_blo = r_blo[torch.flatten(effe_r_ind)]\n",
    "            param_block = param_block[torch.flatten(effe_r_ind)]\n",
    "\n",
    "            rho_i = torch.sum(self.f_r(r_blo, param_block[:, 1][:, [1, 5, 0, 9]])) #* For f^1(r)\n",
    "            f_rho_ = self.f_rho(rho_i, param_block[:, 0][:, [10,11,12,13,14,15,16,17,19,20,2,21,3,18]][0]) #* For F(rho)\n",
    "\n",
    "            self.rho_list[r_ind] = f_rho_\n",
    "\n",
    "        # print(p_e, self.rho_list)\n",
    "        e_all = p_e + torch.sum(self.rho_list)\n",
    "        return e_all\n",
    "\n",
    "def train(model, optimizer, scheduler, path_save, device=None, n = 1000):\n",
    "    # if device is not None:\n",
    "    #     iteration = torch.arange(n).to(device)\n",
    "    # else:\n",
    "    #     iteration = torch.arange(n)\n",
    "    writer = SummaryWriter(log_dir = path_save)\n",
    "    for i in range(n):\n",
    "        loss = model.forward()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # if i % 100 == 0:\n",
    "        #     print(i)\n",
    "        writer.add_scalar(\"Training Loss\", loss, i)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2550214/109256385.py:16: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  self.ind_inter = torch.combinations(torch.arange(self.weights.size(0)), r=2).to(device)\n",
      "2022-10-26 16:50:55.592809: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 16:50:55.712507: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-26 16:50:55.731471: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-26 16:50:56.152325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-26 16:50:56.152362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-26 16:50:56.152365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f74bd79b880>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' \n",
    "Generate the primary CoNi cell in planar FCC lattice\n",
    "'''\n",
    "x_extend, y_extend = 5, 5\n",
    "r_equ = 2.49255140368258\n",
    "cell = supercell_gen(x_extend, y_extend)\n",
    "cell_t = np.array(cell, dtype=bool)\n",
    "init_weight = np.array([math.sqrt(3)/2*r_equ, 1/2*r_equ])\n",
    "coord = np.concatenate([np.where(cell_t)[0].reshape(-1,1),\n",
    "                        np.where(cell_t)[1].reshape(-1,1)], 1)*init_weight\n",
    "\n",
    "coord = torch.from_numpy(coord.astype(np.float32)).clone().requires_grad_()\n",
    "atom_num = len(coord)\n",
    "# coord = torch.rand(atom_num, 2)*torch.sqrt(torch.tensor(atom_num)).requires_grad_()\n",
    "\n",
    "n_co = atom_num//2\n",
    "n_ni = atom_num - n_co\n",
    "\n",
    "''' \n",
    "Embed the corresponding params, also determines the atom specie\n",
    "'''\n",
    "p_nico = torch.tensor([\n",
    "    [2.488746, 2.007018, 27.562015, 27.930410, 8.383453, 4.471175,\n",
    "    0.429046, 0.633531, 0.443599, 0.820658, -2.693513, -0.076445, 0.241442,\n",
    "    -2.375626, -2.7, 0, 0.265390, -0.152856, 0.469, -2.699486, \n",
    "    27.562015*0.85, 27.562015*1.15],\n",
    "    [2.505979, 1.975299, 27.206789, 27.206789, 8.679625, 4.629134,\n",
    "    0.421378, 0.640107, 0.5, 1, -2.541799, -0.219415, 0.733381, -1.589003,\n",
    "    -2.56, 0, 0.705845, -0.687140, 0.694608, -2.559307,\n",
    "    27.206789*0.85, 27.206789*1.15]\n",
    "])\n",
    "ele_co = torch.cat((torch.zeros((n_co, 1)), torch.ones((n_co, 1))), dim=1)\n",
    "ele_ni = torch.cat((torch.ones((n_ni, 1)), torch.zeros((n_ni, 1))), dim=1)\n",
    "ele_list_relax = torch.cat((ele_co, ele_ni), dim=0)\n",
    "shuffle_i = torch.randperm(ele_list_relax.size(0))\n",
    "ele_list = ele_list_relax[shuffle_i]\n",
    "\n",
    "#* Execution\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "param_ = torch.matmul(ele_list, p_nico).to(device)\n",
    "coord.to(device)\n",
    "localtime = time.localtime(time.time())\n",
    "yr_, m_, d_ = localtime[:3]\n",
    "date = f'{yr_}{m_}{d_}_relax'\n",
    "pth = f'/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/pyMD/2D_try/EAM/runs/{date}'\n",
    "\n",
    "m_relax = model(coord, param_).to(device)\n",
    "# Instantiate optimizer\n",
    "opt = torch.optim.Adam(m_relax.parameters(), lr=1e-2)\n",
    "sch = torch.optim.lr_scheduler.StepLR(opt, step_size=1000, gamma = 0.98)\n",
    "\n",
    "losses = train(m_relax, opt, sch, pth, device, n=400)\n",
    "weight_ = m_relax.weights.detach().cpu().numpy()\n",
    "weight_raw = m_relax.weights_.detach().cpu().numpy()\n",
    "np.save(pth+'weight.npy', weight_)\n",
    "np.save(pth+'ele_list.npy', ele_list)\n",
    "\n",
    "plt.scatter(weight_[:, 0], weight_[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, atom_list, param_list, mass_list, v_list, device):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        atom_list: N*dimension\n",
    "        param_list: N*number of params per each atom\n",
    "        per each row:\n",
    "        r_e, f_e, rho_e, rho_s, alpha, beta, A, B, kappa, lamda,\n",
    "        f_n0, f_n1, f_n2, f_n3, f_0, f_1, f_2, f_3, eta, f_e\n",
    "        '''\n",
    "        #* row1: x-coord, row2: y-coord\n",
    "        self.weights = nn.Parameter(atom_list)\n",
    "        self.weights_ = self.weights.clone()\n",
    "        atom_num = len(atom_list)\n",
    "\n",
    "        self.ind_inter = torch.combinations(torch.arange(self.weights.size(0)), r=2).to(device)\n",
    "        ind_inter_f = torch.fliplr(self.ind_inter)\n",
    "        ind_all = torch.cat((self.ind_inter, ind_inter_f), dim=0)\n",
    "        ind_all = ind_all[ind_all[:, 0].sort()[1]]\n",
    "        self.ind_block = torch.split(ind_all, atom_num-1)\n",
    "\n",
    "        self.rho_list = torch.zeros(atom_num).to(device)\n",
    "        self.pe_list = torch.zeros(atom_num).to(device)\n",
    "        self.acc_list = torch.zeros(atom_num, self.weights.size(1)).to(device)\n",
    "        self.v_list = v_list\n",
    "        self.mass = mass_list\n",
    "\n",
    "        self.range = torch.arange(atom_num).to(device)\n",
    "        self.params = param_list\n",
    "        self.opt = torch.optim.Adam([self.weights])\n",
    "\n",
    "    def f_r(self, r, param):\n",
    "        '''\n",
    "        Basis function for f(r), tag 1-2\n",
    "        Param is nD\n",
    "        '''\n",
    "\n",
    "        f_e, beta, r_e, lamda = torch.tensor_split(param, param.size(1), dim=1)\n",
    "        f_e, beta, r_e, lamda = torch.flatten(f_e), torch.flatten(beta), torch.flatten(r_e), torch.flatten(lamda)\n",
    "        \n",
    "        nume = f_e*torch.exp(-beta*(r/r_e-1))\n",
    "        deno = 1 + (r/r_e-lamda)**20\n",
    "        return nume/deno \n",
    "\n",
    "    def phi_r(self, r, param):\n",
    "        ''' \n",
    "        Potential for tag 2-1, atomic pair for same species.\n",
    "        Param is nD.\n",
    "        '''\n",
    "        a, b, r_e, alpha, beta, kappa, lamda = torch.tensor_split(param, param.size(1), dim=1)\n",
    "        a, b = torch.flatten(a), torch.flatten(b)\n",
    "        r_e, alpha, beta = torch.flatten(r_e), torch.flatten(alpha), torch.flatten(beta)\n",
    "        kappa, lamda = torch.flatten(kappa), torch.flatten(lamda)\n",
    "\n",
    "        l_nume = a*torch.exp(-alpha*(r/r_e-1))\n",
    "        l_deno = 1 + (r/r_e-kappa)**20\n",
    "        r_nume = b*torch.exp(-beta*(r/r_e-1))\n",
    "        r_deno = 1 + (r/r_e-lamda)**20\n",
    "\n",
    "        return l_nume/l_deno - r_nume/r_deno\n",
    "\n",
    "    def f_rho(self, rho, param):\n",
    "        ''' \n",
    "        Density function for tag 3-2, param is 1D for the centre atom\n",
    "        '''\n",
    "        f_n0, f_n1, f_n2, f_n3, f_0, f_1, f_2, f_3, f_e, rho_n, rho_e, rho_0, rho_s, eta = param\n",
    "        if rho < rho_n:\n",
    "            return (f_n0 + f_n1*(rho/rho_n-1)\n",
    "                + f_n2*(rho/rho_n-1)**2 + f_n3*(rho/rho_n-1)**3)\n",
    "        elif rho_n <= rho < rho_0:\n",
    "            return (f_0 + f_1*(rho/rho_e-1)\n",
    "                + f_2*(rho/rho_e-1)**2 + f_3*(rho/rho_e-1)**3)\n",
    "        else:\n",
    "            return f_e*(1-torch.log((rho/rho_s)**eta))*(rho/rho_s)**eta\n",
    "\n",
    "    def grad_calc(self,):\n",
    "        ''' \n",
    "        Calculate the whole energy with keeping gradient\n",
    "        '''\n",
    "\n",
    "        for r_ind in self.range:\n",
    "            '''\n",
    "            Extract each atom block containing the centre i and surronding j.\n",
    "            r_ind corresponds to the index of centre atom.\n",
    "            '''\n",
    "            block = self.ind_block[r_ind]\n",
    "            coord_block = self.weights[block]\n",
    "            param_block = self.params[block]\n",
    "\n",
    "            delta_xy = coord_block[:,1]-coord_block[:,0]\n",
    "            r_blo = torch.norm(delta_xy, dim=1)\n",
    "            #* So this will be the cutoff ~3 NN?\n",
    "            effe_r_ind = torch.nonzero(r_blo <= 6)\n",
    "            r_blo = r_blo[torch.flatten(effe_r_ind)]\n",
    "            r_blo.retain_grad()\n",
    "            param_block = param_block[torch.flatten(effe_r_ind)]\n",
    "            delta_xy = delta_xy[torch.flatten(effe_r_ind)]\n",
    "\n",
    "            #* Potential energy\n",
    "            \n",
    "            fr_0 = self.f_r(r_blo, param_block[:, 0][:, [1, 5, 0, 9]]) #* For f^0(r)\n",
    "            fr_1 = self.f_r(r_blo, param_block[:, 1][:, [1, 5, 0, 9]]) #* For f^1(r)\n",
    "            phir_0 = self.phi_r(r_blo, param_block[:, 0][:, [6,7,0,4,5,8,9]]) #* For phi^0(r)\n",
    "            phir_1 = self.phi_r(r_blo, param_block[:, 1][:, [6,7,0,4,5,8,9]]) #* For phi^1(r)\n",
    "            phi_01 = 1/2*(fr_1/fr_0*phir_0 + fr_0/fr_1*phir_1) #* For phi^01\n",
    "            p_e_ = torch.sum(phi_01) #* Potential energy term\n",
    "\n",
    "            #* Electronic density\n",
    "            rho_i = torch.sum(self.f_r(r_blo, param_block[:, 1][:, [1, 5, 0, 9]])) #* For f^1(r)\n",
    "            f_rho_ = self.f_rho(rho_i, param_block[:, 0][:, [10,11,12,13,14,15,16,17,19,20,2,21,3,18]][0]) #* For F(rho)\n",
    "\n",
    "            self.rho_list[r_ind] = f_rho_\n",
    "            self.pe_list[r_ind] = p_e_\n",
    "            e_x = f_rho_ + 1/2*p_e_\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "            e_x.backward(retain_graph = True)\n",
    "            acc = torch.sum(r_blo.grad.reshape(-1,1)*delta_xy/r_blo.reshape(-1,1), dim=0)/self.mass[r_ind] #*N*2\n",
    "            self.acc_list[r_ind] = acc\n",
    "\n",
    "        # print(p_e, self.rho_list)\n",
    "        self.e_total = (torch.sum(self.rho_list) + 1/2*torch.sum(self.pe_list)).detach()\n",
    "        return \n",
    "\n",
    "\n",
    "def train(model, path_save, dt, temp_given, alpha, device=None, n = 1000):\n",
    "\n",
    "    writer = SummaryWriter(log_dir = path_save)\n",
    "    length = len(model.weights)\n",
    "    k_b = constants.k\n",
    "    ev_j = constants.physical_constants['atomic unit of charge'][0]\n",
    "    ''' \n",
    "    v_list: angstrom / s\n",
    "    acc: angstrom / s^2\n",
    "    e_total: eV\n",
    "    kinetic, potential energy in Tensorboard: eV\n",
    "    '''\n",
    "\n",
    "    for i in range(n):\n",
    "        #* Step 1\n",
    "        model.grad_calc()\n",
    "        model.acc_list *= (ev_j*1e20) #* eV -> J\n",
    "        with torch.no_grad():\n",
    "            model.weights += (model.v_list*dt + 1/2*model.acc_list*dt**2) #* x-step\n",
    "        model.v_list += 1/2*model.acc_list*dt\n",
    "        model.v_list -= torch.sum(model.v_list, 0)/length\n",
    "\n",
    "        #* Step 2\n",
    "        model.grad_calc()\n",
    "        model.acc_list *= (ev_j*1e20)\n",
    "        model.v_list += 1/2*model.acc_list*dt\n",
    "        model.v_list -= torch.sum(model.v_list, 0)/length\n",
    "        k_e_ = 1/2*torch.sum(model.mass.reshape(-1,1)*(model.v_list*1e-10)**2)\n",
    "        temp_ = k_e_/(3/2*(length-1))/k_b\n",
    "\n",
    "        writer.add_scalar(\"Potential energy\", model.e_total, i)\n",
    "        writer.add_scalar(\"Kinetic energy\", k_e_/ev_j, i)\n",
    "        writer.add_scalar(\"Temperature\", temp_, i)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            s_adjust = torch.sqrt((temp_given+(temp_-temp_given)*alpha)/temp_)\n",
    "            model.v_list *= s_adjust\n",
    "\n",
    "            # clear_output(True)\n",
    "\n",
    "#! Periodic boundary condition\n",
    "#! Check latent bug (?)\n",
    "#! Memory leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2550214/2756864776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#* Time step, 1 fs = 1e-15 s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m \u001b[0;31m#* For temperature adjusting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mweight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mweight_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2550214/2845748839.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, path_save, dt, temp_given, alpha, device, n)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m#* Step 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_list\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mev_j\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1e20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#* eV -> J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2550214/2845748839.py\u001b[0m in \u001b[0;36mgrad_calc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m             '''\n\u001b[1;32m     86\u001b[0m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind_block\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mcoord_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mparam_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    use_relax = True\n",
    "    ''' \n",
    "    Generate the primary CoNi cell in planar FCC lattice\n",
    "    '''\n",
    "    x_extend, y_extend = 3, 3\n",
    "    r_equ = 2.49255140368258\n",
    "    cell = supercell_gen(x_extend, y_extend)\n",
    "    cell_t = np.array(cell, dtype=bool)\n",
    "    init_weight = np.array([math.sqrt(3)/2*r_equ, 1/2*r_equ])\n",
    "    coord = np.concatenate([np.where(cell_t)[0].reshape(-1,1),\n",
    "                            np.where(cell_t)[1].reshape(-1,1)], 1)*init_weight\n",
    "\n",
    "    if use_relax:\n",
    "        coord = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/pyMD/2D_try/EAM/runs/20221026_relaxweight.npy')\n",
    "\n",
    "    coord = torch.from_numpy(coord.astype(np.float32)).clone().requires_grad_()\n",
    "    atom_num = len(coord)\n",
    "    atom_dim = coord.size(1)\n",
    "\n",
    "    n_co = atom_num//2\n",
    "    n_ni = atom_num - n_co\n",
    "\n",
    "    ''' \n",
    "    Embed the corresponding params, also determines the atom specie\n",
    "    '''\n",
    "    p_nico = torch.tensor([\n",
    "        [2.488746, 2.007018, 27.562015, 27.930410, 8.383453, 4.471175,\n",
    "        0.429046, 0.633531, 0.443599, 0.820658, -2.693513, -0.076445, 0.241442,\n",
    "        -2.375626, -2.7, 0, 0.265390, -0.152856, 0.469, -2.699486, \n",
    "        27.562015*0.85, 27.562015*1.15], #* Ni\n",
    "        [2.505979, 1.975299, 27.206789, 27.206789, 8.679625, 4.629134,\n",
    "        0.421378, 0.640107, 0.5, 1, -2.541799, -0.219415, 0.733381, -1.589003,\n",
    "        -2.56, 0, 0.705845, -0.687140, 0.694608, -2.559307,\n",
    "        27.206789*0.85, 27.206789*1.15] #* Co\n",
    "    ])\n",
    "    m_nico = torch.tensor([ \n",
    "        [58.6934],\n",
    "        [58.9332]\n",
    "    ])*constants.u\n",
    "\n",
    "    ele_co = torch.cat((torch.zeros((n_co, 1)), torch.ones((n_co, 1))), dim=1)\n",
    "    ele_ni = torch.cat((torch.ones((n_ni, 1)), torch.zeros((n_ni, 1))), dim=1)\n",
    "    ele_list = torch.cat((ele_co, ele_ni), dim=0)\n",
    "    shuffle_i = torch.randperm(ele_list.size(0))\n",
    "    ele_list = ele_list[shuffle_i]\n",
    "    if use_relax:\n",
    "        ele_list = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/pyMD/2D_try/EAM/runs/20221026_relaxele_list.npy')\n",
    "        ele_list = torch.from_numpy(ele_list.astype(np.float32))\n",
    "\n",
    "    #* Execution\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    param_ = torch.matmul(ele_list, p_nico)\n",
    "    mass_ = torch.flatten(torch.matmul(ele_list, m_nico))\n",
    "\n",
    "\n",
    "    #* Velocity\n",
    "    temp = 200\n",
    "    k_b = constants.k\n",
    "    v_list = torch.rand(atom_num, atom_dim)*torch.sqrt(3*(1-1/atom_dim)*k_b*temp/mass_.reshape(-1,1))*1e10\n",
    "    v_list -= torch.sum(v_list, 0)/atom_num\n",
    "\n",
    "    #* To device\n",
    "    v_list = v_list.to(device)\n",
    "    mass_ = mass_.to(device)\n",
    "    param_ = param_.to(device)\n",
    "    coord = coord.to(device)\n",
    "\n",
    "\n",
    "    localtime = time.localtime(time.time())\n",
    "    yr_, m_, d_ = localtime[:3]\n",
    "    date = f'{yr_}{m_}{d_}'\n",
    "    pth = f'/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/pyMD/2D_try/EAM/runs/{date}'\n",
    "\n",
    "    m = model(coord, param_, mass_, v_list, device).to(device)\n",
    "\n",
    "    dt = torch.tensor(1e-15).to(device) #* Time step, 1 fs = 1e-15 s\n",
    "    alpha = 0.75 #* For temperature adjusting\n",
    "    train(m, pth, dt, temp, alpha, device, n=100000)\n",
    "    weight_ = m.weights.detach().cpu().numpy()\n",
    "    weight_raw = m.weights_.detach().cpu().numpy()\n",
    "    plt.scatter(weight_[:, 0], weight_[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4190434816\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>memory</th>\n",
       "      <th>convert_memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, memory, convert_memory]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sys import getsizeof\n",
    "def get_memory(threshold=1048576):\n",
    "    '''查看变量占用内存情况\n",
    "\n",
    "    :param threshold: 仅显示内存数值大于等于threshold的变量, 默认为 1MB=1048576B\n",
    "    '''\n",
    "    memory_df=pd.DataFrame(columns=['name', 'memory', 'convert_memory'])\n",
    "    i=0\n",
    "    for key in list(globals().keys()):\n",
    "        memory = eval(\"getsizeof({})\".format(key))\n",
    "        if memory<threshold:\n",
    "            continue\n",
    "        if(memory>1073741824):# GB\n",
    "            unit='GB'\n",
    "            convert_memory=round(memory/1073741824)\n",
    "        elif(memory>1048576):# MB\n",
    "            unit='MB'\n",
    "            convert_memory=round(memory/1048576)           \n",
    "        elif(memory>1024):# KB\n",
    "            unit='KB'\n",
    "            convert_memory=round(memory/1024)  \n",
    "        else:\n",
    "            unit='B' \n",
    "            convert_memory = memory\n",
    "        memory_df.loc[i]=[key, memory, str(convert_memory)+unit]\n",
    "        i=i+1\n",
    "    # 按照内存占用大小降序排序    \n",
    "    memory_df.sort_values(\"memory\",inplace=True,ascending=False)\n",
    "    return memory_df\n",
    "    \n",
    "memory_df = get_memory()\n",
    "memory_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68475d8e8ba7c27bff5b0c1dcce162ecdafd8f583568d2d03f898fe272d0ccc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
